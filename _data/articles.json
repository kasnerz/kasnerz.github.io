[
	{
		"title": "TabGenie: A Toolkit for Table-to-Text Generation",
		"code": "https://github.com/kasnerz/tabgenie",
		"authors": "Zdeněk Kasner, Ekaterina Garanina, Ondřej Plátek, Ondřej Dušek",
		"venue": "arXiv",
		"text": "We began developing this tool to play with the generative language models in real time, but it soon evolved into a swiss knife for table-to-text generation. TabGenie provides interactive data visualization, unified data representation & unified programming interface for more than 15 datasets. You can use the web interface as a dataset viewer and a model playground, the programming interface then allows  to quickly prototype new models.",
		"img": "2023_tabgenie.png",
		"arxiv": "https://arxiv.org/abs/2302.14169",
		"demo": "https://quest.ms.mff.cuni.cz/rel2text/tabgenie/",
		"poster": "/assets/papers/pdf/2023_tabgenie_poster.pdf",
		"id": "tabgenie"
	},
	{
		"title": "Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models",
		"code": "https://github.com/kasnerz/rel2text",
		"authors": "Zdeněk Kasner, Ioannis Konstas, Ondřej Dušek",
		"venue": "EACL 2023",
		"text": "What is a better way to learn the data semantics – memorizing an arbitrary mapping or taking the human-readable data labels into account? On the task of describing a triple <i>(entity_1, relation, entity_2)</i>, we show that the models are able to describe previously unseen relations as long as the relation label is meaningful and unambiguous. To put it another way: if you want to train robust data-to-text systems, don't use abbreviations!",
		"img": "2022_rel2text.png",
		"arxiv": "https://arxiv.org/abs/2210.07373",
		"acl": "https://aclanthology.org/2023.eacl-main.176/",
		"poster": "/assets/papers/pdf/2023_mind_the_labels_poster.pdf",
		"id": "rel2text"
	},
	{
		"title": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
		"acl": "https://aclanthology.org/2022.acl-long.271/",
		"arxiv": "https://arxiv.org/abs/2203.16279",
		"poster": "/assets/papers/pdf/2022_zeroshot_pipeline_poster.pdf",
		"code": "https://github.com/kasnerz/zeroshot-d2t-pipeline",
		"authors": "Zdeněk Kasner, Ondřej Dušek",
		"venue": "ACL 2022",
		"text": "To combine the power of pretrained language models with controllability of pipeline approaches, we formulate data-to-text generation as a sequence of trainable text-to-text operations: ordering, aggregation, and paragraph compression. As a welcome side-effect, we get rid of semantically incorrect outputs arising from noisy human-written references. Is NL-only approach to data-to-text generation the way to go?",
		"img": "2022_zeroshot_pipeline.png",
		"id": "neural_pipeline"
	},
	{
		"title": "Text-in-Context: Token-Level Error Detection for Table-to-Text Generation",
		"award": "Best submission at Shared Task in Evaluating Accuracy",
		"acl": "https://aclanthology.org/2021.inlg-1.25/",
		"poster": "/assets/papers/pdf/2021_text_in_context_poster.pdf",
		"code": "https://github.com/kasnerz/accuracySharedTask_CUNI-UPF",
		"authors": "Zdeněk Kasner, Simon Mille, Ondřej Dušek",
		"venue": "INLG 2021",
		"text": "How to automatically detect which parts of the generated text do not correspond to the data? In our submission for the <a href=\"https://github.com/ehudreiter/accuracySharedTask\">Shared Task in Evaluating Accuracy 2021</a>, we devise a 3-step approach combining a rule-based system with pretrained language models. Our approach was the best out of four submitted metrics!",
		"img": "2021_text_in_context.png",
		"id": "text_in_context"
	},
	{
		"title": "Train Hard, Finetune Easy: Multilingual Denoising for RDF-to-Text Generation",
		"award": "2nd place in Russian RDF-to-text generation",
		"acl": "https://www.aclweb.org/anthology/2020.webnlg-1.20/",
		"authors": "Zdeněk Kasner, Ondřej Dušek",
		"venue": "INLG 2020",
		"text": "<i>Data</i> == <i>noisy text</i>. That's definitely an overgeneralization, oversimplification... but um, it works! We succesfully generate text from DBPedia data in <i>English</i> and <i>Russian</i> just by finetuning mBART – a pretrained multilingual denoising autoencoder. This is our submission for the <a href=\"https://webnlg-challenge.loria.fr/challenge_2020/\">WebNLG Challenge 2020</a>, presented at the <a href=\"https://webnlg-challenge.loria.fr/workshop_2020/\">3rd Workshop on Natural Language Generation from the Semantic Web.</a>",
		"img": "2020_mbart.png"
	},
	{
		"title": "Evaluating Semantic Accuracy of Data-to-Text Generation with Natural Language Inference",
		"award": "INLG 2020 Best Short Paper Award",
		"acl": "https://www.aclweb.org/anthology/2020.inlg-1.19/",
		"arxiv": "https://arxiv.org/abs/2011.10819",
		"code": "https://github.com/ufal/nlgi_eval",
		"authors": "Ondřej Dušek, Zdeněk Kasner",
		"venue": "INLG 2020",
		"text": "Does a text contain all the information from the data? Hard to check manually, even harder to code. But what if we reformulate the question a little bit: can we <i>infer</i> all the data from the text and nothing else? That sounds like natural language inference. And guess what - somebody already pretrained a neural model for that!",
		"img": "2020_nli_inlg.png",
		"id": "semacc"
	},
	{
		"title": "Data-to-Text Generation with Iterative Text Editing",
		"acl": "https://www.aclweb.org/anthology/2020.inlg-1.9/",
		"arxiv": "https://arxiv.org/abs/2011.01694",
		"poster": "/assets/papers/pdf/2020_d2t_text_editing.pdf",
		"code": "https://github.com/kasnerz/d2t_iterative_editing",
		"authors": "Zdeněk Kasner, Ondřej Dušek",
		"venue": "INLG 2020",
		"text": "Imagine your task is to generate text from data. Which sounds easier: <i>generating text from scratch</i> or <i>joining existing sentences</i>? We propose an approach in which we iteratively join the sentences with a text-editing neural model. Since the model has a limited vocabulary, it has also a limited possibilities of introducing incorrect facts. Moreover, it also turns out that sentence fusion is a quite general task which works on multiple domains.",
		"img": "2020_d2t_text_editing.png",
		"id": "iterative_editing"
	},
	{
		"title": "Improving Fluency of Non-Autoregressive Machine Translation",
		"arxiv": "https://arxiv.org/abs/2004.03227",
		"code": "https://github.com/kasnerz/neuralmonkey-ctc-decoder",
		"authors": "Zdeněk Kasner, Jindřich Libovický, Jindřich Helcl",
		"venue": "arXiv",
		"text": "In the follow-up of my master thesis, we improve translation quality of a CTC-based machine translation model. The model is non-autoregressive, i.e. faster but lacking behind autoregressive models in translation quality. To improve the translation quality, we re-score the hypotheses during the beam search decoding with an n-gram language model and several other features.",
		"img": "2019_nonautoregressive.png"
	}
]