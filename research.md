---
layout: article
title: Research
permalink: /research/
---

I work on **natural language generation** with **pretrained neural language models**. I focus on building systems which can accurately describe structured data (graphs, tables, charts, etc.) from various domains. Here is an illustration of how such "data-to-text generation" may look like:

<img src="/assets/d2t.png" alt="d2t" style="max-width: 90%; margin: auto;">

In my research, I combine the best of template-based systems and pretrained neural language models: using templates for **transforming the data to text** and using models such as BART or T5 for **postprocessing the text** (see [[1]](#neural_pipeline), [[2]](#iterative_editing), [[3]](#text_in_context)).



My other research interests (ranging from "I digged into that a little bit" up to "if only I had more time") include:
- evaluating quality of generated texts,
- low-resource natural language generation,
- story generation,
- logical and commonsense reasoning,
- cognitive neuroscience and language processing in human brain.


## Selected publications
See my **<img src="/assets/icons/scholar.png" style="display: inline"> [Google Scholar](https://scholar.google.cz/citations?user=6NnuRB8AAAAJ)** profile for the full list of my publications.