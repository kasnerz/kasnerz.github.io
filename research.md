---
layout: article
title: Research
permalink: /research/
---
In my research, I am working on **natural language generation** with **pretrained neural language models**. I focus on building systems which can accurately describe structured data (graphs, tables, charts, etc.) from various domains.

<img src="/assets/d2t.png" alt="d2t" style="max-width: 90%; margin: auto;">

Existing systems are usually built on hand-crafted templates, and recent pretrained language models such as BART or T5 could enable making these systems more flexible. However, these models are not good in manipulating with data. Therefore, I try to combine the best of both worlds: using templates for **transforming the data to text** and using neural models for **postprocessing the text** (see [[1]](#neural_pipeline), [[2]](#iterative_editing), [[3]](#text_in_context)).


My other research interests (ranging from "I digged into that a little bit" up to "if only I had more time") include:
- evaluating quality of generated texts,
- low-resource natural language generation,
- story generation,
- logical and commonsense reasoning,
- cognitive neuroscience and language processing in human brain.


## Selected publications
For the full list of my publications, have a look at my **<img src="/assets/icons/scholar.png" style="display: inline"> [Google Scholar](https://scholar.google.cz/citations?user=6NnuRB8AAAAJ) profile**.