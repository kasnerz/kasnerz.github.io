---
layout: publication
title: Research
permalink: /research/
---
## Research interests

In my research, I am working on **natural language generation** with **pretrained neural language models**. I focus on building systems which can accurately describe structured data (graphs, tables, charts, etc.) from various domains: 

<img src="/assets/nlg_da_scheme.png" alt="d2t" style="max-width: 90%; margin: auto;">

So far, the area is dominated by single-purpose template-based systems. A way to make the systems applicable to multiple domains could be using pretrained neural language models such as BART or T5. However, these models are good in language generation, but not in manipulating with data. My approach (see [[1]](#neural_pipeline), [[2]](#iterative_editing), [[3]](#text_in_context)) is combining the best of both worlds: using templates for **transforming the data to text**, and using neural models for **improving the text quality**.


My other research interests (ranging from "I digged into that a little bit" up to "if only I had more time") include:
- evaluating quality of generated texts
- low-resource natural language generation
- story generation
- logical and commonsense reasoning
- cogntive neuroscience and language processing in human brain.


## Selected publications
For the full list of my publications, have a look at my **<img src="/assets/icons/scholar.png" style="display: inline"> [Google Scholar](https://scholar.google.cz/citations?user=6NnuRB8AAAAJ) profile**.