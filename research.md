---
layout: article
title: Research
permalink: /research/
---
I work on **natural language generation** with **pretrained neural language models**. I focus on building systems which can accurately describe structured data (graphs, tables, charts, etc.) from various domains.

<img src="/assets/d2t.png" alt="d2t" style="max-width: 90%; margin: auto;">

Existing systems are usually built on hand-crafted templates. Pretrained language models such as BART or T5 have potential for making these systems more flexible, but the models are not good in manipulating with data. 

In my research, I combine the best of both worlds: using templates for **transforming the data to text** and using neural models for **postprocessing the text** (see [[1]](#neural_pipeline), [[2]](#iterative_editing), [[3]](#text_in_context)).


My other research interests (ranging from "I digged into that a little bit" up to "if only I had more time") include:
- evaluating quality of generated texts,
- low-resource natural language generation,
- story generation,
- logical and commonsense reasoning,
- cognitive neuroscience and language processing in human brain.


## Selected publications
For the full list of my publications, have a look at my **<img src="/assets/icons/scholar.png" style="display: inline"> [Google Scholar](https://scholar.google.cz/citations?user=6NnuRB8AAAAJ)** profile.