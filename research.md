---
layout: article
title: Research
permalink: /research/
---

In my research, I try to ensure that **large language models (LLMs) generate accurate** text.

The projects I have worked on include:
- domain-independent and low-resource data-to-text generation [[1]](#d2t-llm), [[2]](#neural_pipeline), [[3]](#iterative_editing),
- evaluating semantic accuracy of generated text [[4]](#text_in_context), [[5]](#semacc), [[8]](#factgenie), [[9]](#d2t-llm), [[10]](#llm-span-annot)
- investigating the role of data labels [[6]](#rel2text).
- a software toolkit for data-to-text generation [[7]](#tabgenie).

During my internship at [Mila](https://mila.quebec/), I was working on applying LLMs for autonomous web navigation.

More broadly, I am also interested in **model interpretability**: how is the information inside the language models represented, how do language models reason, and how this all relates to human cognition.

## Selected publications
<!-- See my **<img src="/assets/icons/scholar.png" style="display: inline"> [Google Scholar](https://scholar.google.cz/citations?user=6NnuRB8AAAAJ)** profile for the full list of my publications. -->