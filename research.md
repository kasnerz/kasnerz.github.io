---
layout: article
title: Research
permalink: /research/
---
In my research, I am working on **natural language generation** with **pretrained neural language models**. I focus on building systems which can accurately describe structured data (graphs, tables, charts, etc.) from various domains: 

<img src="/assets/nlg_da_scheme.png" alt="d2t" style="max-width: 90%; margin: auto;">

Existing systems are usually built on hand-crafted templates. A way to make the systems more flexible could be using pretrained models such as BART or T5. However, these models are good in language generation, but not in manipulating with data. 

My approach (see [[1]](#neural_pipeline), [[2]](#iterative_editing), [[3]](#text_in_context)) is to combine the best of both worlds: using templates for **transforming the data to text**, and using neural models for **improving the text quality**.


My other research interests (ranging from "I digged into that a little bit" up to "if only I had more time") include:
- evaluating quality of generated texts
- low-resource natural language generation
- story generation
- logical and commonsense reasoning
- cogntive neuroscience and language processing in human brain.


## Selected publications
For the full list of my publications, have a look at my **<img src="/assets/icons/scholar.png" style="display: inline"> [Google Scholar](https://scholar.google.cz/citations?user=6NnuRB8AAAAJ) profile**.